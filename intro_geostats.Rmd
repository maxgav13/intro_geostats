---
title: "Introducción al análisis geoestadístico de datos en geociencias: teoría y aplicación"
author: 
  - name: "Maximiliano Garnier-Villarreal"
    affiliation: 'Escuela Centroamericana de Geología, Universidad de Costa Rica'
    email: 'maximiliano.garniervillarreal@ucr.ac.cr'
affiliation:
  - id            : "1"
    institution   : "Escuela Centroamericana de Geología, Universidad de Costa Rica"
keywords: ['Geoestadística','Kriging','R','Variograma','Interpolación','Validación cruzada']
abstract: |
  Kriging, el método de interpolación asociado a geoestadística, se ha usado y ha sido propuesto como el mejor método de interpolación, muchas veces sin realmente entender cómo es que se usa adecuadamente y dejando que el software que lo brinda decida cómo implementarlo. Esta aseveración tiene fundamento cuando se procede de la manera correcta, realizando los pasos necesarios durante el análisis y modelado geoestadístico, por lo que es necesario entender cómo aplicar Kriging correctamente para que los resultados obtenidos sean relevantes y confiables. Estos pasos se detallan en este trabajo, abordando la teoría, y mediante un ejemplo, con datos de la temperatura promedio de los últimos 10 para el 8 de Marzo para la provincia de San José, se pone en práctica el método usando el software estadístico libre **R**. Adicionalmente, se presenta una aplicación web de libre acceso para quienes no se sientan cómodos usando lenguajes de programación.
# date: "`r format(Sys.Date(), '%d %B %Y')`"
lang: es
bibliography: ["bib/all.bib"]
# biblio-style: apalike2
csl: csl/apa6.csl
css: css/style.css
link-citations: true
# documentclass: "apa6"
# classoption: "man"
output:
  bookdown::word_document2:
    reference_docx: 'template_RGAC.docx'
    df_print: kable
    toc: false
    toc_depth: 2
    number_sections: false
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    number_sections: true
    keep_md: true
    # dev: "pdf"
  bookdown::pdf_document2:
    df_print: kable
    number_sections: false
    toc: false
    includes:
      in_header: header.tex
  distill::distill_article:
    toc: true
    toc_depth: 3
    toc_float: true
    df_print: kable
  papaja::apa6_word: default
always_allow_html: true
---

```{r setup, include=FALSE}
library(here)
library(summarytools)
library(knitr)
library(raster)
library(gstat)
library(sp)
library(sf)
library(stars)
library(mapview)
library(viridis)
# library(rgeos)
# library(rgdal)
library(DescTools)
library(RColorBrewer)
library(ggrepel)
library(MOTE)
library(papaja)
library(kableExtra)
library(rio)
library(patchwork)
library(janitor)
# library(conflicted)
library(tidymodels)
library(tidyverse)

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  fig.path = "figures/",
  fig.retina = 3,
  fig.width = 6,
  fig.asp = 0.618,
  fig.align = "center",
  out.width = "90%"
)

options(OutDec = ',')

theme_set(theme_bw(base_size = 12, base_family = 'Arial'))
# conflict_prefer('select','dplyr')
# conflict_prefer('filter','dplyr')

x_var = 'Distancia [km]'
y_var = 'Semivarianza'
x_map = 'X [km]'
y_map = 'Y [km]'
x_vmap = "Distancia E-W [km]"
y_vmap = "Distancia N-S [km]"
```

# Introducción {#geostats-intro}

En las ciencias que tienen una fuerte componente espacial (dentro de ellas geología) es común recolectar muestras, describirlas y tener la ubicación de dónde se recolectaron. En muchos casos el muestreo se hace con el fin de caracterizar una variable o proceso en el espacio, con lo que se tiene en mente pasar de puntos a una superficie (mapa).

Para poder generar estas superficies se pueden emplear diferentes [Métodos de interpolación](#geostats-met-interp), donde comúnmente se ha dado a entender que Kriging es el método por excelencia a usar (casi que indiscriminadamente), pero muchos usuarios no saben cómo usar el método apropiadamente y cuándo es adecuado o no utilizarlo.

La facilidad que brindan programas de cómputo comerciales (Surfer, ArcGIS), con sus interfaces "point & click", de implementar éste y otros métodos hace creer al usuario que es simplemente de escoger un método y decirle que lo ejecute, sin guiar al usuario de manera apropiada en el proceso necesario para obtener resultados significativos, confiables, y reproducibles. Cabe mencionar que el procedimiento y los pasos que se van a mostrar acá están disponibles en estos softwares pero no de manera frontal para el usuario.

El objetivo principal de este trabajo es de índole educativo/informativo y corresponde con introducir al lector en qué es la geoestadística y cómo realizar un análisis geoestadístico básico de manera apropiada. La idea es brindar una base y guía de cómo hacer una interpolación de los datos de interés y en español, ya que la mayoría de los textos (libros y artículos) están en inglés, y a veces se enfocan únicamente en los resultados (mapas) y no tanto en el proceso.

Para el procesamiento de los datos y la implementación de la geoestadística se va a utilizar el software estadístico libre multi-plataforma **R** [@R-base] así como diferentes paquetes, que va a permitir el desarrollo de rutinas que se pueden reutilizar para análisis futuros. Adicionalmente se presentará una aplicación web, desarrollada en **R** y de libre acceso, que hace uso de lo expuesto aquí. Se recomienda al lector, si no está familiarizado con **R** o quiere profundizar más en su uso, consultar @garnier-villarreal2020.

# Métodos de interpolación {#geostats-met-interp}

De manera resumida y sin entrar en mucho detalle se mencionan diferentes métodos de interpolación comúnmente usados, para ellos se puede consultar @webster2007. De manera general se tienen: Polígonos de Thiessen, Triangulación, Vecinos naturales (natural neighbours), Inverso de la distancia (inverse distance), Superficies de tendencia (trend surface), Ajuste polinomial (splines), y **Kriging**.

El método de Kriging es lo que más se asocia con la geoestadística, y va a ser el énfasis de lo aquí presentado. El Kriging es considerado como el método más robusto y preciso, de ahí que en inglés es conocido como **blue** que quiere decir **b**est **l**inear **u**nbiased **e**stimator, y se puede traducir como **mejor estimador lineal no-sesgado** [@isaaks1989; @webster2007].

Una ventaja de Kriging con respecto a otros métodos de interpolación más populares, es que a parte de estimar el valor de la variable de interés, estima además un error de la interpolación, lo que permite tener una idea de la calidad (incertidumbre) de los resultados [@isaaks1989; @webster2007]. El método ha sido utilizado para predecir la intensidad sísmica [@linkimer2008rgac], el nivel de agua subterránea [@varouchakis2012hsj], pérdida de suelo [@wang2003pers], y temperatura del aire [@wang2017rs], entre otras.

# Geoestadística {#geostats-basico}

La geoestadística no es estadística (clásica) aplicada a datos geológicos, es un tipo de estadística que hace uso de la componente espacial de los datos y pretende caracterizar sistemas distribuidos en el espacio los cuales no se conocen por completo [@davis2002; @isaaks1989; @webster2007]. Hay que resaltar que Kriging es un método de interpolación (uno de los usos de la geoestadística) que corresponde con uno de los pasos en el análisis y modelado geoestadístico [@oliver2014c], no hay que confundir o pensar que geoestadística es lo mismo que Kriging, que es un error común.

La geoestadística (Kriging) se ha utilizado más para la interpolación (estimación - Kriging) de variables en el espacio, pero también se puede utilizar para la simulación (Simulación Gaussiana Secuencial) de la variable de interés (otra forma de usar geoestadística que no es Kriging). El resultado de la interpolación es la distribución del valor promedio de la variable (cuál sería el valor más probable de encontrar), la simulación genera una cantidad definida de realizaciones (N) de la variable, que pueden estar condicionadas o no a datos observados, y presentan una distribución más heterogénea que la interpolación [@chiles1999; @goovaerts1997; @pebesma2020; @pyrcz2014; @webster2007]. En este trabajo el enfoque va a ser en el uso más común y sencillo que es la interpolación (estimación) de una variable en el espacio.

La base de lo que se va a exponer corresponde con capítulos de @davis2002, @swan1995, @borradaile2003, y @mckillup2010, y textos más detallados y exclusivos en la materia de @chiles1999, @cressie1993, @goovaerts1997, @isaaks1989, @pyrcz2014, @webster2007, y @wackernagel2003, los cuales corresponden con referencias clásicas y actualizadas. Para la implementación en **R** y más base teórica y práctica se puede consultar @nowosad2019 y @pebesma2020.

A continuación se definen algunos conceptos fundamentales en geoestadística, que forman las bases teórica y práctica para el análisis geoestadístico.

## Correlación espacial

El concepto fundamental en geoestadística y la estadística espacial en general, es que las observaciones son inter-dependientes en función de la distancia entre ellas, donde hay más similitud (relación) conforme más cercanas estén las observaciones y esa similitud o relación es más débil conforme la distancia incrementa [@chiles1999; @cressie1993; @goovaerts1997; @isaaks1989; @webster2007].

## Semivarianza

Esta es la medida que se usa para determinar la disimilitud (relación) entre observaciones que varían con la distancia, y se representa mediante la Ecuación \@ref(eq:semivarianza), donde $Z(x_i)$ es el valor de la variable en la posición $x_i$, $Z(x_i+h)$ es el valor de la variable a una distancia $h$, $N$ es el número total de puntos (observaciones), y $N(h)$ es el número de pares de puntos que se encuentran a una distancia $h$ específica. **Se recomienda tener más de 30 pares de puntos por cada distancia** $h$, y no calcular la semivarianza más allá de la mitad de la máxima distancia entre observaciones [@chiles1999; @goovaerts1997; @isaaks1989; @webster2007].

```{=tex}
\begin{equation}
  \gamma(h) = \frac{1}{2N(h)}\sum_{i=1}^{N(h)} [Z(x_i+h)-Z(x_i)]^2
  (\#eq:semivarianza)
\end{equation}
```
Si los datos se encuentran ordenados en una grilla regular se puede usar la separación entre puntos como las diferentes distancias $h$ (Figura \@ref(fig:semivar) (a) y (b)). Si los datos se encuentran irregularmente espaciados es necesario agruparlos en franjas (Figura \@ref(fig:semivar) (c)), donde se requiere definir una tolerancia de la distancia ($w$, por lo general $h/2$), y una tolerancia angular ($\alpha/2$) [@oliver2014c; @webster2007].

(ref:semivar) Esquema del calculo de la semivarianza para datos regularmente espaciados, donde los datos están completos (a) y donde hay datos faltantes (b); para datos irregularmente espaciados (c). Modificado de @webster2007.

```{r semivar, echo=FALSE, fig.cap='(ref:semivar)'}
knitr::include_graphics(here('images','F01.png'))
```

## Variograma experimental

Para visualizar la relación (o no) entre la semivarianza y la distancia (relación espacial de la variable) se usa el variograma experimental (Figura \@ref(fig:variograma)).

El cálculo de la semivarianza y su representación por medio del variograma experimental son los primeros pasos donde el usuario/analista tiene control sobre la construcción y representación de la relación espacial de la variable, y el resultado va a ser el insumo para pasos posteriores. *Como decisiones fundamentales se tienen la escogencia de la distancia máxima y el intervalo de distancias (*$h$). Conforme se varíen estos valores va a variar la semivarianza, cualquier ajuste que se le realice, y su posterior uso en la interpolación [@isaaks1989; @oliver2014c; @webster2007].

(ref:variograma) Ejemplo de variogramas experimentales: **A** Mostrando la relación (dependencia) espacial de la variable, **B** Mostrando la ausencia de relación (dependencia) espacial de la variable.

```{r variograma, echo=FALSE, fig.cap='(ref:variograma)'}
knitr::include_graphics(here('images','F02.png'))
```

## Modelo de variograma

El variograma experimental es una representación discreta de la relación espacial ya que se cuenta solo con puntos a las distancias definidas. Para poder interpolar valores a diferentes distancias es necesario tener un modelo continuo que se ajuste a los datos. Para ajustar un modelo hay que analizar el variograma experimental y realizar una estimación inicial de las partes o parámetros que lo van a definir [@goovaerts1997; @sarma2009; @webster2007].

### Partes

La partes o parámetros que definen a un modelo de variograma se muestran en la Figura \@ref(fig:modelo-variog), y son [@isaaks1989; @sarma2009; @webster2007]:

(ref:modelo-variog) Modelo de variograma mostrando las partes: meseta, pepita, y rango. Modificado de @webster2007.

```{r modelo-variog, echo=FALSE, fig.cap='(ref:modelo-variog)'}
knitr::include_graphics(here('images','F03.png'))
```

-   Meseta total ($S$, sill en inglés): Valor del variograma o semivarianza cuando la distancia $h$ tiende a infinito (cuando la semivarianza se estabiliza), y por lo general es muy similar al valor de la varianza de la variable de interés.

-   Meseta parcial ($C_1$, partial sill en inglés): La diferencia entre la meseta total y la pepita ($C_1 = S - C_0$). Si no hubiera pepita ($C_0=0$), entonces $C_1 = S$.

-   Pepita ($C_0$, nugget en inglés): El intercepto, el valor de la semivarianza en el origen, y representa por lo general una discontinuidad del variograma en el origen, que se puede deber a la escala de muestreo o errores de medición.

-   Rango ($a$, range en inglés): El límite del área de influencia, es la distancia a partir del cual el variograma se estabiliza y se alcanza la meseta; a partir de esta distancia las observaciones se consideran independientes (sin relación).

### Modelos

Aquí se exponen los principales tipos de modelos que se usan en geociencias [@goovaerts1997; @isaaks1989; @sarma2009; @webster2007]. La Figura \@ref(fig:variog-modelos) muestra la forma de estos diferentes modelos, junto con sus partes.

(ref:variog-modelos) Modelos más usados en geociencias: **A** Potencia, **B** Esférico, **C** Exponencial, **D** Gaussiano. Modificado de @sarma2009.

```{r variog-modelos, eval=TRUE, echo=FALSE, fig.cap='(ref:variog-modelos)', out.width='90%'}
knitr::include_graphics(here('images','F04.png'))
```

-   Potencia

Es más usado cuando el variograma no se estabiliza o alcanza una meseta. Se calcula mediante la Ecuación \@ref(eq:variog-potencia), donde $\alpha$ es la pendiente, $0<\lambda<2$ y controla la concavidad o convexidad del modelo. Un ejemplo se muestra en la Figura \@ref(fig:variog-modelos) A.

```{=tex}
\begin{equation}
  \gamma(h) = C_0 + \alpha h^{\lambda}
  (\#eq:variog-potencia)
\end{equation}
```

(ref:variog-potencia) Modelo de potencia. Tomado de @sarma2009.

```{r variog-potencia, eval=FALSE, echo=FALSE, fig.cap='(ref:variog-potencia)', out.width='50%'}
knitr::include_graphics(here('images','variog-potencia.png'))
```

-   Esférico

Es de los más usados en geociencias, presenta una meseta definida, y se caracteriza por presentar un comportamiento lineal cerca del origen. Se calcula mediante la Ecuación \@ref(eq:variog-esferico), y un ejemplo se muestra en la Figura \@ref(fig:variog-modelos) B.

```{=tex}
\begin{equation}
  \gamma(h) = 
  \begin{cases}
  C_0 + C_1 \left[ \frac{3}{2}\left( \frac{h}{a}\right) - \frac{1}{2}\left( \frac{h}{a}\right)^3 \right] & \text{para } h < a\\
  C_0 + C_1 & \text{para } h > a
  \end{cases}
  (\#eq:variog-esferico)
\end{equation}
```

(ref:variog-esferico) Modelo esférico. Tomado de @sarma2009.

```{r variog-esferico, eval=FALSE, echo=FALSE, fig.cap='(ref:variog-esferico)', out.width='50%'}
knitr::include_graphics(here('images','variog-esferico.png'))
```

-   Exponencial

Este modelo tiene un comportamiento asintótico y no alcanza una meseta tan estable como el esférico, por esto lo que se usa en el modelo como rango es $r=a/3$, o sea, una tercera parte del rango esperado. Se calcula mediante la Ecuación \@ref(eq:variog-exp) y un ejemplo se muestra en la Figura \@ref(fig:variog-modelos) C.

```{=tex}
\begin{equation}
  \gamma(h) = C_0 + C_1 \left[ 1 - exp\left(-\frac{h}{r}\right) \right]
  (\#eq:variog-exp)
\end{equation}
```

(ref:variog-exp) Modelo exponencial. Tomado de @sarma2009.

```{r variog-exp, eval=FALSE, echo=FALSE, fig.cap='(ref:variog-exp)', out.width='50%'}
knitr::include_graphics(here('images','variog-exp.png'))
```

-   Gaussiano

Este modelo es similar al exponencial en que no alcanza una meseta estable sino que tiene un comportamiento asintótico, y otra característica es que tiene un comportamiento suavizado cerca del origen. Como no alcanza una meseta el rango que se usa en el modelo es $r=a/\sqrt{3}$, o sea, el rango esperado entre la raíz de 3. Se calcula mediante la Ecuación \@ref(eq:variog-gaus), y un ejemplo se muestra en la Figura \@ref(fig:variog-modelos) D.

```{=tex}
\begin{equation}
  \gamma(h) = C_0 + C_1 \left[ 1 - exp\left(-\frac{h}{r}\right)^2 \right]
  (\#eq:variog-gaus)
\end{equation}
```

(ref:variog-gaus) Modelo gaussiano. Tomado de @sarma2009.

```{r variog-gaus, eval=FALSE, echo=FALSE, fig.cap='(ref:variog-gaus)', out.width='50%'}
knitr::include_graphics(here('images','variog-gaus.png'))
```

La Figura \@ref(fig:variog-comparacion) es una comparación de los tres modelos más comunes en geociencias, donde todos corresponden con una estructura que presenta los siguientes parámetros: $C_0=0$, $C_1=30$, y $a=210$. Hay que resaltar que el modelo esférico tiene un comportamiento lineal cerca del origen, el modelo exponencial un comportamiento más creciente (convexo), y el modelo gaussiano un comportamiento suavizado. Adicionalmente, los modelos exponencial y gaussiano no alcanzan la meseta de la estructura, contrario al esférico que sí la alcanza.

(ref:variog-comparacion) Comparación visual de los tres modelos más usados en geociencias, todos representando la misma estructura ($C_0=0$, $C_1=30$, y $a=210$).

```{r variog-comparacion, echo=FALSE, fig.cap='(ref:variog-comparacion)', out.width='90%'}
knitr::include_graphics(here('images','F05.png'))
```

## Anisotropía

La variable y su relación en el espacio puede no solo depender de la distancia sino también de la dirección en que se estima. Si hay una dependencia (comportamiento diferenciado) de la dirección se dice que existe una anisotropía y sino el comportamiento es isotrópico u omnidireccional. La anisotropía se representa como una elipse, donde el eje mayor va a alinearse con la dirección de mayor continuidad espacial y el eje menor con la dirección de menor continuidad espacial (perpendicular al eje mayor) [@chiles1999; @goovaerts1997; @isaaks1989; @oliver2014c; @webster2007]. Lo acostumbrado es escoger direcciones en el rango de 0 a 180, ya que al tratarse de un elipse las direcciones mayores a 180 son simplemente el opuesto de direcciones menores a 180 (ejemplo: 220 es el opuesto de 40).

La anisotropía puede ser de dos tipos: *geométrica* o *zonal.* La geométrica es la más común y la más fácil de modelar. En la anisotropía geométrica se tiene, para las diferentes direcciones, la misma meseta pero diferente rango. En la anisotropía zonal se tiene el mismo rango pero mesetas diferentes [@chiles1999; @goovaerts1997; @isaaks1989; @webster2007].

Para determinar la presencia o no de anisotropía se pueden usar el mapa de la superficie de variograma (Figura \@ref(fig:variog-anis) **A**) y/o variogramas direccionales (Figura \@ref(fig:variog-anis) **B**). La *anisotropía geométrica* va a presentar una dirección principal (eje mayor) que va a estar orientada en la dirección que presenta el mayor rango (mayor continuidad espacial), y una dirección menor (eje menor) orientada perpendicularmente a la principal [@chiles1999; @goovaerts1997; @isaaks1989; @webster2007].

En la Figura \@ref(fig:variog-anis) la dirección principal coincide con los 35° y la menor con los 125°. En los diferentes softwares por lo general se expresa la anisotropía como una razón y va a depender del software cuál va en el numerador y cuál en el denominador. En el caso del paquete **gstat** la razón de anisotropía va a tener en el numerador la dirección menor y en el denominador la dirección mayor, por lo que la razón va a tener un rango de 0 a 1, donde mientras más cercano a 0 el valor mayor va a ser la anisotropía.

(ref:variog-anis) **A** Ejemplo de mapa de la superficie de variograma, mostrando anisotropía donde el eje principal ocurre en la dirección 35° y el eje menor ocurre en la dirección 125°. **B** Variogramas direccionales donde se observa como en la dirección de 35 se alcanza un rango mayor ($\sim 25^\circ$), mientras que en la dirección perpendicular (125°) el rango es menor ($\sim 15^\circ$)

```{r variog-anis, echo=FALSE, fig.cap='(ref:variog-anis)', out.width='80%'}
knitr::include_graphics(here('images','variog-anis.png'))
```

## Validación cruzada

Dado que que objetivo de la interpolación es predecir valores en puntos donde no se tiene información, la mejor forma de evaluar el ajuste de un modelo específico sobre el variograma experimental es por medio de la validación cruzada. De manera general lo que se hace es dejar por fuera una o varias de las observaciones, se re-ajusta el modelo seleccionado, y se predice el valor de la variable para esas observaciones que se dejaron por fuera, repitiendose el proceso hasta tener una predicción para todos los puntos [@chiles1999; @goovaerts1997; @isaaks1989; @oliver2014c; @webster2007; @hastie2008; @james2013; @kuhn2013; @witten2011].

El tipo de validación cruzada más usado es *LOO* (leave-one-out), donde se deja por fuera una observación a la vez, se re-ajusta el modelo y se predice el valor de la variable para cada observación por separado [@goovaerts1997; @isaaks1989; @oliver2014c; @webster2007]. El paquete **gstat** ofrece esta opción (por defecto) y la opción de *K-Fold*. En *K-Fold* se escoge una cantidad de grupos (*K*) en los que se dividen las observaciones (típicamente 5 o 10) y se deja un grupo de observaciones por fuera cada vez, se re-ajusta el modelo, se predice el valor de la variable para todas las observaciones del grupo que se dejó por fuera, y este proceso se repite *K* veces hasta tener predicciones para todos los puntos [@hastie2008; @james2013; @kuhn2013; @witten2011].

Una vez realizado el ajuste y la validación cruzada del modelo se obtienen valores predichos y observados para cada punto. Con esta información se pueden usar diferentes métricas, donde lo ideal sería comparar cada una de estas métricas para diferentes modelos ajustados, y se escogería el modelo que obtenga mejores métricas [@chiles1999; @goovaerts1997; @isaaks1989; @oliver2014c; @webster2007].

Dentro de las métrica más usadas están [@oliver2014c; @webster2007; @yao2013po]:

En estas métricas $N$ es el total de observaciones (puntos), $Y_i$ es el valor observado en el punto $i$, $\hat{Y_i}$ es el valor predicho en el punto $i$, $s^2_{ei}$ es el error/varianza de la predicción, y $\bar{Y}$ es la media (promedio) de la variable.

-   Error medio ($ME$): El error corresponde con los residuales de lo observado menos lo predicho, una vez se tienen estos valores se les calcula la media e idealmente se esperaría obtener un valor cercano a 0. Se calcula mediante la Ecuación \@ref(eq:xval-me) y al comparar modelos se escogería el modelo que presente un valor más cercano a 0.

```{=tex}
\begin{equation}
  ME = \frac{1}{N} \sum_{i=1}^{N} (Y_i-\hat{Y_i})
  (\#eq:xval-me)
\end{equation}
```
-   Error cuadrático medio ($RMSE$): Este valor corresponde con la desviación promedio de los errores al cuadrado. Se encuentra en la escala de la variable e idealmente se prefieren valores pequeños. Se calcula mediante la Ecuación \@ref(eq:xval-rmse) y comparando modelos se escogería el modelo que presente un $RMSE$ menor.

```{=tex}
\begin{equation}
  RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (Y_i-\hat{Y_i})^2}
  (\#eq:xval-rmse)
\end{equation}
```
-   Razón de desviación cuadrática media ($MSDR$): Esta valor compara la diferencia entre la predicción y valor actual con respecto a la varianza (error) obtenida de la interpolación ($s^2_{ei}$). Se esperaría que este valor ande cerca de 1. Se calcula mediante la Ecuación \@ref(eq:xval-msdr) y comparando modelos se escogería el que presente un $MSDR$ más cercano a 1.

```{=tex}
\begin{equation}
  MSDR = \frac{1}{N} \sum_{i=1}^{N} \frac{(Y_i-\hat{Y_i}^2)}{s^2_{ei}}
  (\#eq:xval-msdr)
\end{equation}
```
-   Error Porcentual Absoluto Medio ($MAPE$): Es una medida porcentual de la diferencia entre lo observado y lo predicho, con un rango de 0 a 1 o de 0 a 100 si se multiplica por 100. Se esperaría que este valor ande cerca de 0 o lo más bajo posible. Se calcula mediante la Ecuación \@ref(eq:xval-mape) y comparando modelos se escogería el que presente el $MAPE$ más bajo.

```{=tex}
\begin{equation}
  MAPE = \frac{1}{N} \sum_{i=1}^{N} \Big| \frac{(Y_i-\hat{Y_i})}{Y_i} \Big|
  (\#eq:xval-mape)
\end{equation}
```
-   Estadístico de bondad de predicción ($G$): Este estadístico mide qué tan efectiva es la predicción a si se hubiera usado simplemente la media (promedio) de la variable. Valores de 1 indican una predicción perfecta, valores positivos indican que el modelo es más efectivo que usar la media, valores negativos indican que el modelo es menos efectivo que usar la media, y un valor de cero indica que sería mejor usar la media. Se calcula mediante la Ecuación \@ref(eq:xval-g) y comparando modelos se escogería el que presente el $G$ más cercano a 1 o más positivo.

```{=tex}
\begin{equation}
  G = 1 -  \bigg[ \frac{\sum_{i=1}^{N}(Y_i-\hat{Y_i})^2}{\sum_{i=1}^{N}(Y_i-\bar{Y})^2} \bigg]
  (\#eq:xval-g)
\end{equation}
```
Todas estas métricas, excepto la $MSDR$, se pueden aplicar para cualquier modelo de cualquier método de interpolación. Para la $MSDR$ se ocupa que el método brinde un error (varianza) de la predicción y ésta es una de las fortalezas de Kriging sobre la mayoría de métodos. Como recomendación, al comparar modelos si hay valores muy similares de la mayoría de las métricas se recomienda usar las métricas de $MSDR$ y el estadístico $G$ como las más importantes.

## Kriging

Kriging es un método de interpolación (estimación), por lo que la idea es obtener valores de la variable en lugares donde no se pudo medir. El método hace uso del modelo ajustado para asignar pesos a los puntos a interpolar dependiendo de la distancia entre ellos. Los puntos más cercanos van a presentar valores menores de semivarianza (mayor peso) y los puntos más lejanos valores mayores de semivarianza (menor peso), y si hay puntos que caen fuera del rango estos van a tener una influencia mínima o nula [@chiles1999; @goovaerts1997; @isaaks1989; @webster2007]. Lo anterior se presenta de manera gráfica en la Figura \@ref(fig:kriging-pesos).

(ref:kriging-pesos) Visualización del proceso de interpolación mediante Kriging, donde para el punto a interpolar (D), el punto que está más cercano (C) tiene más peso (influencia, baja semivarianza), y el punto más lejano (A) prácticamente no tiene peso ya que cae fuera del rango. Tomado de @mckillup2010.

```{r kriging-pesos, echo=FALSE, fig.cap='(ref:kriging-pesos)'}
knitr::include_graphics(here('images','kriging-pesos.png'))
```

Dentro de las ventajas del Kriging están que compensa por efectos de agrupamiento (clustering) al dar menos peso individual a puntos dentro del agrupamiento que a puntos aislados, y da una estimación de la variable y del error (varianza de Kriging) [@chiles1999; @goovaerts1997; @isaaks1989; @trauth2015; @webster2007]. El resultado de la interpolación por medio Kriging, por lo general, suaviza los resultados, y sobre-estima valores pequeños y sub-estima valores grandes [@oliver2014c; @webster2007].

Kriging es un método general con diferentes variantes dependiendo de la información que se tenga, el tipo de variable, y la cantidad y tipos de variables a considerar. A manera más general también puede incorporar información temporal, por lo que se puede determinar y modelar la variación espacio-temporal de la variable o variables. **Es más recomendado usar Kriging cuando los datos están normalmente distribuidos, se tiene una buena cantidad de observaciones (depende pero 30, 40 o más es lo recomendado), son estacionarios (la media y varianza de la variable no varían significativamente, esto puede subsanarse con diferentes variantes), y hay una dependencia espacial de la variable (variograma muestra un incremento de la semivarianza con la distancia)** [@chiles1999; @goovaerts1997; @isaaks1989; @webster2007].

Los tipos de Kriging más comunes son [@chiles1999; @goovaerts1997; @isaaks1989; @webster2007]:

-   *Simple (*$SK$): Para esta variante se asume que se conoce la media de la variable (lo cual no es necesariamente cierto), y que la media es constante. En general no es práctico de usar.
-   *Ordinario (*$OK$): Esta variante es la más usada, donde se asume una media constante pero desconocida, y adicionalmente los datos no deben presentar una tendencia.
-   *Lognormal (*$OK_{log}$): Esta variante se usa cuando la variable tienen una fuerte asimetría positiva, donde se aplica el logaritmo a los datos, y sobre estos datos log-transformados se aplica el Kriging Ordinario; lo más común es usar el logaritmo natural. **Para obtener el resultado de la interpolación en la escala original de la variable NO es tan simple como exponenciar los resultados. @cressie1993, @webster2007, @laurent1963jasa, y @yamamoto2007cg brindan más detalles de cómo realizar la transformación inversa de la manera más apropiada.**
-   *Universal (*$UK$): Esta variante aplica cuando la media no es constante y no se conoce; se le conoce también como *Kriging con tendencia (Kriging in the presence of a trend)*. Esta es una forma de trabajar cuando los datos presentan una tendencia (típicamente en función de las coordenadas), como es el caso típico de niveles piezométricos. @lark2006ejss brinda más detalles y técnicas más actualizadas de como lidiar con este tipo de situación.
-   *CoKriging (*$CK$): Esta variante se usa cuando se quiere utilizar la información de 2 o más variables, y corresponde con la versión multivariable de Kriging. Es necesario que haya una relación entre las variables y su relación espacial, lo que se conoce como co-regionalización.
-   *Indicador (*$IK$): Esta variante se usa cuando la variable es cualitativa (categórica) o se transforma una variable cuantitativa en cualitativa para determinar si la variable excede o no un umbral. El resultado es la probabilidad condicional de cada una de las categorías (niveles) de la variable.

@eldeiry2010jide, @kravchenko1999a, @meng2013cagis, @wang2017rs, y @yao2013po hacen uso de varios de los tipos de Kriging, así como de otros métodos de interpolación, describiendo brevemente los métodos y comparando los resultados entre ellos.

# Análisis geoestadístico {#geostats-analisis}

Una vez presentada la teoría básica de la geoestadística se va a proceder a realizar un análisis geoestadístico típico (con el objetivo de estimar la distribución de la variable de interés en el espacio). Los datos corresponden con la temperatura promedio de los últimos 10 años para el 8 de Marzo para la provincia de San José. Los datos fueron tomados de @meteomatics2021, de donde se pueden obtener diferentes parámetros meteorológicos/climáticos a nivel mundial. Se usan estos datos por tener amplia cobertura espacial, lo que los hacen muy didácticos para ejemplificar el uso de la geoestadística.

Se va a hacer uso de **R** que permite manipular y analizar datos (espaciales y no espaciales), y además tiene diversos paquetes (librerías) para realizar análisis geoestadísticos [@finley2015jss; @jing2015jss; @ribeiro2003p3iwdsc; @R-gstat; @gstat2004; @gstat2016]. El código usado, así como su explicación, se pueden encontrar en el material extra disponible en el repositorio de GitHub del trabajo: <https://github.com/maxgav13/intro_geostats>.

```{r data, eval=FALSE, include=FALSE}
set.seed(0987)
x <- 1:100 # x coordinates
y <- 1:100 # y coordinates
dat <- expand.grid(x = x, y = y) # create data frame with all combinations
dat$z <- 1 # initialize z variable
coordinates(dat) <- ~x + y # set coordinates
gridded(dat) <- TRUE # specify data is gridded

modelo = vgm(psill = 0.9, model = 'Sph', range = 30,
             nugget = 0.1) # model to simulate
beta.mod = 30 # mean of the random field
var.mod = sum(modelo$psill) # variance of the random field

g1 <- gstat(id = 'z', formula = z~1, model = modelo,
           data = dat, dummy = TRUE, beta = beta.mod, nmax = 50) # create gstat object
dat.1 <- predict(g1, newdata = dat, nsim = 1)
dat.1.df = as_tibble(dat.1) # converts simulation to data frame

datos = slice_sample(dat.1.df, n = 60) %>% 
  rename(z = sim1)

export(datos, here('data','datos.csv')) # exports point data
```

## Análisis Exploratorio de Datos

Antes de iniciar con el análisis geoestadístico es necesario estudiar la variable, ver su distribución (si se aproxima a una distribución normal) para determinar si es necesaria alguna transformación, y por medio de la varianza se puede tener una idea aproximada de la meseta total del variograma.

(ref:AED) Histograma de la temperatura promedio de los últimos 10 años para el 8 de Marzo para la provincia de San José. La línea roja corresponde con la media, y la curva azul con la curva de densidad empírica.

```{r AED, echo=FALSE, fig.cap='(ref:AED)'}
datos = import(here('data','clima_SJ_tidy.csv'), setclass = 'tbl')

myvar = 'TempC' # variable a modelar
myvar.lab = 'Temperatura [°C]'

# descr(datos[myvar],style = 'rmarkdown') %>% 
#   as.data.frame() %>% 
#   mutate(Estadistica = row.names(.)) %>% 
#   relocate(Estadistica) %>% 
#   slice(-c(12,13,15)) %>% 
#   kable(col.names = c('Estadística','Valor'),
#         digits = 2,
#         format = 'simple',
#         caption = 'Resumen estadístico de la variable "z".')

descr(datos[myvar],style = 'rmarkdown',transpose = T) %>% 
  select(-c(5,7,10,12,13,15)) %>% 
  relocate(N.Valid) %>% 
  kable(col.names = c('N','Media','Desv. Est.','Min','Mediana',
                      'Max','MAD','CV','Asimetría'),
        digits = 2,
        # format = 'simple',
        align = 'c',
        caption = 'Resumen estadístico de los datos.') %>% 
  kable_styling(full_width = F)

S = var(datos[[myvar]]) # varianza de la variable

gg.hist = ggplot(datos, aes_string(myvar)) + 
  geom_histogram(aes(y = stat(density)), bins = 10, 
                 col = 'black', fill = 'orange') + 
  geom_vline(xintercept = mean(datos[[myvar]]), col = 'red', size=.75) +
  geom_density(col = 'blue', size=.75) +
  labs(x = myvar.lab, y = 'Densidad')
gg.hist
```

El resumen estadístico (Cuadro \@ref(tab:AED)) y el histograma (Figura \@ref(fig:AED)) muestran que tiene una distribución aproximadamente normal, donde la media y mediana son similares y el histograma presenta una forma general de campana con una asimetría inferior a 1, por lo que no es necesaria ninguna transformación. La varianza de la variable es `r apa(S,3)`, lo que brinda una aproximación de la meseta total del variograma.

En este caso los datos tienen coordenadas geográficas pero de manera general se recomienda trabajar los datos en sistemas de coordenadas planas (x,y) por lo que se se recomienda convertirlas a estas conforme la zona de estudio, utilizando los códigos *epsg* respectivos. En este caso el código que corresponde es el *5367* para el sistema de coordenadas *CRTM05*. Para más información al respecto se puede consultar @garnier-villarreal2020, donde el capítulo 6 está dedicado al trato de datos espaciales en **R**.

```{r datos-sf}
datos_sf = st_as_sf(datos, coords = c('Long','Lat'), crs = 4326) %>% 
  st_transform(crs = 5367) %>% 
  mutate(X = st_coordinates(.)[,1], Y = st_coordinates(.)[,2]) %>% 
  relocate(X, Y)
datos_sp = as(datos_sf, 'Spatial')
coordnames(datos_sp) = c('X','Y')
```

```{r distancias}
dists = st_distance(datos_sf) %>% .[lower.tri(.)] %>% unclass()
distancias = signif(c(min(dists), mean(dists), max(dists)),3)/1000 # rango de distancias
names(distancias) = c('min', 'media', 'max') 
# distancias
```

Es buena práctica determinar las distancias entre los puntos, ya que como se explicó en la parte teórica, no es recomendado calcular el variograma experimental a más de la mitad de la distancia máxima entre puntos. Haciendo este paso se obtiene que la distancia máxima es de `r apa(distancias[3],2)` km. Dada la zona de estudio tan grande se van a presentar las distancias en kilómetros para mayor facilidad y legibilidad, pero hay que tener en cuenta que los datos se encuentran en metros.

```{r outline, include=FALSE}
outline = st_read('data/SJ.gpkg',layer = 'SJ_poly')
```

```{r grilla-interp}
bb = st_bbox(datos_sf)
dint = max(c(bb[3]-bb[1],bb[4]-bb[2])/nrow(datos_sf))
dx = seq(bb[1],bb[3],dint) # coordenadas x
dy = seq(bb[4],bb[2],-dint) # coordenadas y
st_as_stars(matrix(0, length(dx), length(dy))) %>%
  st_set_dimensions(1, dx) %>%
  st_set_dimensions(2, dy) %>%
  st_set_dimensions(names = c("X", "Y")) %>% 
  st_set_crs(st_crs(datos_sf)) -> datosint

datosint2 = st_crop(datosint, outline)
```

Un paso inicial antes de empezar con el análisis es visualizar la distribución de la variable en el espacio, para tener una idea preliminar de patrones que pueda presentar. La Figura \@ref(fig:dist-espacial) muestra la ubicación de los datos, donde los puntos se encuentran rellenados de acuerdo al valor de la temperatura, y donde se puede observar que hay una predominancia de temperaturas mayores al SW y menores al NE.

(ref:dist-espacial) Mapa de puntos mostrando la distribución espacial de la temperatura promedio de los últimos 10 años para el 8 de Marzo para la provincia de San José.

```{r dist-espacial, fig.cap='(ref:dist-espacial)'}
gg.map.pts = ggplot() + 
  geom_sf(data = outline, col = 'cyan', alpha = .1, size = .75) + 
  geom_sf(data = datos_sf, aes_string(col = myvar), size = 3, alpha = 0.6) + 
  scale_color_viridis_c() + 
  scale_x_continuous(labels = scales::label_number(scale = 1/1000)) +
  scale_y_continuous(labels = scales::label_number(scale = 1/1000)) +
  labs(x = x_map, y = y_map, col = myvar.lab) +
  if (!is.na(st_crs(datos_sf))) {
    coord_sf(datum = st_crs(datos_sf))
  }
gg.map.pts
```

## Modelado geoestadístico

Habiendo estudiado la variable y hecho los pasos iniciales de manipulación, análisis y visualización se procede con el modelado geoestadístico, mostrando y explicando los distintos pasos. En este caso como la variable no requirió de ninguna transformación se va a usar el Kriging Ordinario.

### Variograma experimental

El primer paso es crear un variograma experimental omnidireccional. Se va a hacer uso del paquete **gstat** [@R-gstat; @gstat2004; @gstat2016] para la geoestadística. En la construcción del variograma experimental se deben definir los argumentos de el intervalo de distancia deseado ($h$), y la distancia máxima a la cual calcular la semivarianza. Si recordamos la distancia máxima era `r apa(max(distancias),2)` km, por lo que se escoge un valor ligeramente inferior a la mitad (70 km) para la distancia máxima y un valor de 4 km para el intervalo de distancias $h$. Es en este paso donde el usuario puede probar diferentes valores para obtener un variograma representativo, que muestre una estructura de dependencia espacial y que los puntos del variograma se hayan calculado con suficientes datos (recomendable 20 o más).

```{r eval=TRUE, echo=FALSE}
myformula = as.formula(paste(myvar,'~1'))
g = gstat(formula = myformula, 
          data = datos_sf) # objeto gstat para hacer geoestadistica

# variograma experimental cada cierta distancia (width), y hasta cierta distancia (cutoff)
dat.vgm = variogram(g, 
                    width = 4000,
                    cutoff = 70000) 
```

(ref:variog-omni) Variograma experimental omnidireccional de la temperatura promedio de los últimos 10 años para el 8 de Marzo para la provincia de San José. Las etiquetas de los puntos muestran con el número de pares de puntos usados para el cálculo de la semivarianza. La línea roja punteada corresponde con la varianza de la variable, que es una aproximación a la meseta total.

```{r variog-omni, fig.cap='(ref:variog-omni)', out.width='90%'}
gg.omni.exp = ggplot(dat.vgm,aes(x = dist, y = gamma)) + 
  geom_point(size = 2) + 
  labs(x = x_var, y = y_var) +
  geom_hline(yintercept = S, col = 'red', linetype = 2) +
  scale_x_continuous(labels = scales::label_number(scale = 1/1000),
                     limits = c(0, max(dat.vgm$dist))) +
  geom_text_repel(aes(label = np), size = 2)
gg.omni.exp
```

Una vez analizado el variograma omnidireccional se procede a determinar si existe la presencia o no de anisotropía. Para esto se usan tanto el mapa de la superficie de variograma (Figura \@ref(fig:variog-map)), como los variogramas direccionales (Figura \@ref(fig:variog-dir)).

```{r eval=TRUE, echo=FALSE}
map.vgm <- variogram(g, 
                     width = 8000, 
                     cutoff = 70000, 
                     map = TRUE)
```

(ref:variog-map) Mapa de la superficie de variograma. Se observa una dirección preferencial a aproximadamente 135°.

```{r variog-map, fig.cap='(ref:variog-map)', out.width='90%'}
gg.map.exp = ggplot(data.frame(map.vgm), aes(x = map.dx, y = map.dy, fill = map.var1)) +
  geom_raster() + 
  scale_fill_gradientn(colours = plasma(20)) +
  labs(x = x_vmap, y = y_vmap, fill = "Semivarianza") +
  coord_equal()
gg.map.exp
```

Para los variogramas direccionales hay que definir, adicionalmente, los argumentos de las direcciones y la tolerancia angular, donde lo más usado son direcciones cada 45° y la tolerancia angular es la mitad del intervalo entre direcciones (22,5°). De nuevo, solo es necesario definir direcciones entre 0 y 180, y 180 se excluye por ser el opuesto de 0.

```{r eval=TRUE, echo=FALSE}
# con direcciones y tolerancia angular
d = c(0,45,90,135) # direcciones
dat.vgm2 = variogram(g, 
                     width = 4000,
                     cutoff = 70000,
                     alpha = d,
                     tol.hor = 22.5) 
```

```{r}
dat.vgm2.gg = dat.vgm2 %>% 
  mutate(dir.hor = factor(dir.hor, labels = as.character(d)))
```

(ref:variog-dir) Variogramas experimentales direccionales cada 45°. La línea roja punteada representa la varianza de la variable, lo que se aproxima a la meseta total. Se observa un mayor rango en la dirección 135° y un menor rango en la dirección 45°.

```{r variog-dir, fig.cap='(ref:variog-dir)', out.width='100%', fig.height=6, fig.width=7}
gg.dir.exp = ggplot(dat.vgm2.gg,aes(x = dist, y = gamma,
                       col = dir.hor, shape = dir.hor)) + 
  geom_point(size = 2) + 
  labs(x = x_var, y = y_var, col = "Dirección", shape = 'Dirección') +
  geom_hline(yintercept = S, col = 'red', linetype = 2) +
  ylim(0, max(dat.vgm2$gamma)) +
  scale_x_continuous(labels = scales::label_number(scale = 1/1000), 
                     limits = c(0, max(dat.vgm2$dist))) +
  scale_color_brewer(palette = 'Dark2') +
  scale_shape_manual(values = 1:4) + 
  facet_wrap(~dir.hor) + 
  geom_text_repel(aes(label = np), size = 2, show.legend = F) +
  theme(legend.position = 'top')
gg.dir.exp
```

Analizando el mapa y los variogramas direccionales se concluye que hay una anisotropía con dirección principal de 135° y un rango aproximado de 50 km, y un rango aproximado de 25 km en la dirección de 45°, resultando en una razón de anisotropía de 0,5. Por lo anterior el modelado se realizará con los variogramas direccionales.

### Ajuste de modelo de variograma

Una vez creado el variograma experimental es necesario ajustarle un modelo para poder obtener valores a distancias no muestreadas. Antes de ajustar un modelo al variograma experimental es necesario estimar las partes del mismo (meseta, pepita, rango, anisotropía) y determinar valores iniciales, para posteriormente realizar el ajuste.

Usando los variogramas direccionales (Figura \@ref(fig:variog-dir)), se puede estimar una pepita de aproximadamente 0, una meseta parcial de 25, un rango de 50000, una anisotropía a 135° con una razón de 0,5, y se puede usar un modelo tipo esférico ('Sph').

```{r params, echo=FALSE}
pep = 0 # pepita
meseta = 25 # meseta parcial
mod = "Sph" # modelo a ajustar (esférico)
rango = 50000 # rango
anis = c(135,.5)
```

```{r ajuste, eval=TRUE, echo=FALSE}
dat.fit = fit.variogram(dat.vgm, 
                        model = vgm(psill = meseta, 
                                    model = mod, 
                                    range = rango, 
                                    nugget = pep,
                                    anis = anis))

fit.rmse = sqrt(attributes(dat.fit)$SSErr/(nrow(datos))) # error del ajuste
```

El modelo ajustado (Cuadro \@ref(tab:ajuste-tab)) se puede usar para calcular un error del ajuste inicial ($RMSE_{ajuste}=`r apa(fit.rmse,4)`$), pero es más confiable el que se obtiene usando la validación cruzada, ya que el obtenido acá es un valor optimista. Lo anterior se da puesto que se calcula con respecto a los datos que se utilizaron para el ajuste (toda la información disponible) y esto siempre va a resultar en error menor que cuando se usa el modelo en datos no observados [@hastie2008; @james2013; @kuhn2013; @witten2011], que es el objetivo de la interpolación.

```{r eval=TRUE}
varmod = dat.fit # modelo ajustado
```

```{r ajuste-tab, echo=FALSE}
varmod %>% 
  select(1:5) %>% 
  kable(col.names = c('Modelo','Meseta','Rango',
                      'Razón','Dirección Principal'),
        format = 'simple',
        align = 'c',
        digits = 3,
        caption = 'Modelo ajustado') %>% 
  kable_styling(full_width = F)
```

El modelo ajustado del Cuadro \@ref(tab:ajuste-tab) se puede interpretar así: el efecto pepita ('Nug') (que como es el intercepto solo aporta información a la semivarianza y no al rango) aporta `r apa(varmod$psill[1],3)` a la semivarianza ($C_0=`r apa(varmod[1,2],3)`$); el modelo esférico ('Sph') aporta `r apa(varmod$psill[2],3)` a la semivarianza ($C_1=`r apa(varmod[2,2],3)`$), con lo que la meseta total es $S=C_0+C_1=`r apa(sum(varmod[1:2,2]),3)`$, y el rango mayor del modelo esférico es $a=`r apa(varmod[2,3],2)`$ en una dirección 135°, con una razón de anisotropía de 0,5.

Con el modelo ajustado se puede visualizar este sobre el variograma omnidireccional (Figura \@ref(fig:ajuste-1)) y los variogramas direccionales (Figura \@ref(fig:ajuste-2)). En general, para todos los casos se observa que el modelo ajustado es válido y representativo para todos los casos.

```{r}
unit_vector = function(th) {
  th = if_else(th>=180, th-180, th)
  if (th == 0) {
    uv = c(0,1,0)
  } else if (th == 90) {
    uv = c(1,0,0)
  } else if (th == 180) {
    uv = c(0,-1,0)
  } else if (between(th,0,90)) {
    uv = c(sin(th*pi/180),cos(th*pi/180),0)
  } else if (between(th,90,180)) {
    uv = c(cos((th-90)*pi/180),-1*sin((th-90)*pi/180),0)
  }
  return(uv)
}
```

```{r}
variog.dir = map_dfr(d, ~variogramLine(object = varmod, 
                                       maxdist = max(dat.vgm$dist),
                                       min = 0.001, n = 100, 
                                       dir = unit_vector(.x)), 
                     .id = 'dir.hor') %>% 
  as_tibble() %>% 
  mutate(dir.hor = factor(dir.hor, labels = as.character(d)))
```

(ref:ajuste-1) Variograma experimental omnidireccional con el modelo esférico ajustado sobrepuesto.

```{r ajuste-1, fig.cap='(ref:ajuste-1)', out.width='90%'}
# plot(dat.vgm, dat.fit, xlab = x_var, ylab = y_var) 

# omnidireccional

gg.omni.fit = variog.dir %>% 
  filter(dir.hor == 0) %>%
  ggplot(aes(dist,gamma)) + 
  geom_point(data = dat.vgm,shape=3,size=2) +
  geom_line(size=1)  +
  labs(x = x_var, y = y_var) +
  scale_x_continuous(labels = scales::label_number(scale = 1/1000)) +
  coord_cartesian(ylim = c(0,max(dat.vgm$gamma)))
gg.omni.fit
```

(ref:ajuste-2) Variogramas experimentales direccionales con el modelo esférico ajustado sobrepuesto, mostrando que el modelo es válido.

```{r ajuste-2, fig.cap='(ref:ajuste-2)', out.width='100%', fig.height=6, fig.width=7}
# plot(dat.vgm2, dat.fit, as.table = T, xlab = x_var, ylab = y_var) 

# direccionales

gg.dir.fit = variog.dir %>% 
  ggplot(aes(dist,gamma,col=dir.hor)) + 
  geom_point(data = dat.vgm2.gg,shape=3,size=1.5) +
  geom_line(size=1) + 
  scale_color_brewer(palette = 'Dark2') +
  scale_x_continuous(labels = scales::label_number(scale = 1/1000)) +
  labs(x = x_var, y = y_var, col = 'Dirección') +
  facet_wrap(~dir.hor) +
  theme(legend.position = 'top')
gg.dir.fit
```

### Validación cruzada

Para evaluar de manera más realista el ajuste de cualquier modelo es mejor usar la validación cruzada. Es en este paso que se podrían probar diferentes modelos, donde se obtienen las métricas de ajuste de un modelo, se ajusta un nuevo modelo y se obtienen sus métricas de ajuste, y así iterativamente. Una vez ajustados diferentes modelos y con sus diferentes métricas, se puede tener un criterio más robusto de cuál modelo se ajusta mejor a los datos.

Las métricas usadas aquí son las que se introdujeron anteriormente: el error cuadrático medio ($RMSE$), la razón de desviación cuadrática media ($MSDR$), el error porcentual absoluto medio ($MAPE$), y el estadístico de bondad de predicción ($G$). Adicionalmente se estima la correlación ($r$) entre los valores observados y predichos, donde lo que se busca es determinar qué tan similares son los valores entre si (Figura \@ref(fig:xval-plots) **A**).

```{r xval, eval=TRUE, include=FALSE}
kcv.ok = krige.cv(myformula, 
                  locations = datos_sf, 
                  model = varmod)
```

```{r xval-metrics, include=TRUE}
cl = .95 # nivel de confianza
decimales = 3 # decimales a usar

xval.rmse = sqrt(mean(kcv.ok$residual^2)) # RMSE - menor es mejor

xval.msdr = mean(kcv.ok$residual^2/kcv.ok$var1.var) # MSDR - ~1 es mejor

xval.mod = lm(observed ~ var1.pred, as.data.frame(kcv.ok))

xval.r2 = xval.mod %>% 
  broom::glance() %>% 
  pull(r.squared) # r2 - mayor es mejor

xval.g = 1 - (sum((kcv.ok$var1.pred-kcv.ok$observed)^2)/sum((kcv.ok$observed-mean(kcv.ok$observed))^2)) # G - mayor y positivo es mejor

xval.mape = MAPE(xval.mod) # MAPE - menor es mejor

correl = signif(CorCI(cor(kcv.ok$observed, 
                          kcv.ok$var1.pred), 
                      nrow(kcv.ok)),
                decimales) # r

metricas = tibble(metric = c('$RMSE$','$MSDR$','$r$','$R^2$','$MAPE$','$G$'), 
                  estimate = c(apa(xval.rmse,decimales),
                               apa(xval.msdr,decimales),
                               apa(correl[1],decimales,F),
                               apa(xval.r2,decimales,F),
                               apa(xval.mape,decimales,F),
                               apa(xval.g,decimales,F)))
```

```{r xval-metrics-tab, echo=FALSE}
metricas %>% 
  kable(col.names = c('Métrica','Valor'),
        format = 'simple',
        align = 'c',
        # digits = 3,
        caption = 'Métricas de ajuste para la validación cruzada') %>% 
  kable_styling(full_width = F)
```

Como se mencionó arriba las métricas son más útiles cuando se comparan modelos, pero para este caso, usando solo el modelo esférico, se puede decir que presentan valores aceptables (Cuadro \@ref(tab:xval-metrics-tab)): la $MSDR$ está cerca de 1, la correlación ($r$) es alta, el $RMSE$ es menor a la desviación estándar de los datos (`r apa(sqrt(S),3,T)`), el $MAPE$ es bajo y cercano a 0, y el estadístico $G$ es positivo y cercano a 1. Conforme APA -@americanpsychologicalassociation2010, valores que no pueden por definición ser superiores a 1 o inferiores a -1 ($r$, $R^2$, $MAPE$, y $G$) se reportan sin el '0' inicial.

Si recordamos el error del ajuste inicial sobre los datos que se realizó el ajuste fue $RMSE_{ajuste}=`r apa(fit.rmse,4)`$, que como se puede observar es mucho menor al error de la validación cruzada $RMSE_{xval}=`r apa(xval.rmse,3)`$, de ahí que se le definiera como optimista, y sea el error de la validación cruzada un mejor indicador de la capacidad predictiva del modelo seleccionado.

```{r xval-plots1, fig.cap='Relación entre los valores observados y predichos por la validación cruzada. La línea roja es la línea 1:1 y la línea verde es la regresión entre los datos.', out.width='80%'}
gg.xval1 = ggplot(as.data.frame(kcv.ok), aes(var1.pred, observed)) + 
  geom_point(col = "blue", shape = 3, size = 1.25) + 
  coord_fixed() + 
  geom_abline(slope = 1, col = "red", size = .75) + 
  geom_smooth(se = F, method = 'lm', col = 'green', size = .75) +
  labs(x = "Predichos", y = "Observados")
```

```{r xval-plots2, fig.cap='Histograma de los residuales de la validación cruzada. La linea roja es la media de los residuales y la curva azul la curva de densidad.', out.width='80%'}
gg.xval2 = ggplot(as.data.frame(kcv.ok), aes(residual)) + 
  geom_histogram(aes(y=stat(density)), bins = 8, 
                 col = 'black', fill = "orange") + 
  geom_density(col = 'blue', size=.75) + 
  labs(x = "Residuales", y = "Densidad") + 
  geom_vline(xintercept = mean(kcv.ok$residual), col = 'red', size=.75)
```

(ref:xval-plots) Análisis de los resultados de validación cruzada. **A** Relación entre los valores observados y predichos por la validación cruzada. La línea roja es la línea 1:1 y la línea verde es la regresión entre los datos. **B** Histograma de los residuales de la validación cruzada. La línea roja es la media de los residuales y la curva azul la curva de densidad.

```{r xval-plots, fig.cap='(ref:xval-plots)', out.width='80%'}
gg.xval = gg.xval1 + gg.xval2 + 
  plot_annotation(tag_levels = 'A')
gg.xval
```

Adicionalmente se pueden explorar los residuales ya que idealmente se esperaría que presenten una distribución normal centrada en 0. Lo anterior se puede apreciar en la Figura \@ref(fig:xval-plots) **B**, donde el histograma es aproximadamente normal, y no presenta una asimetría importante (menor a 1: `r apa(Skew(kcv.ok$residual),3)`) y se encuentra centrado alrededor de 0.

Las métricas tanto como los residuales indican que el modelo ajustado es un modelo apropiado para proceder con la interpolación.

## Interpolación (Kriging)

Para recalcar nuevamente, el análisis geoestadístico es un proceso que conlleva el cálculo del variograma, el ajuste de un modelo, la validación del modelo a usar, y por último la interpolación mediante Kriging. Si no se realizan con cuidado los pasos el resultado de la interpolación puede no tener validez o sentido.

```{r kriging, eval=TRUE, include=FALSE}
ok = krige(myformula, 
           locations = datos_sf,
           newdata = datosint2, 
           model = varmod)
```

Los mapas finales tanto de la predicción (estimación) como de la varianza (error de estimación) se presentan en la Figura \@ref(fig:mapas-kriging). En el mapa de la predicción se observa una tendencia de valores altos de la temperatura hacia al SW del área y de valores bajos de temperatura hacia el NE, con una zonación con dirección de rumbo paralela al eje mayor de la anisotropía. Esta distribución espacial de la temperatura tiene sentido puesto que la costa (clima más cálido) se encuentra en dirección SW, y las partes montañosas (clima más fresco) de la meseta central en la dirección NE. El mapa de la varianza va a presentar los valores más bajos en los puntos de muestreo y valores mayores en puntos más distantes de los muestreados, que es un comportamiento típico de Kriging.

```{r mapa-pred, warning=FALSE, fig.cap='Mapa de predicción de la variable de interés.', out.width='90%'}
gg.pred = ggplot() + 
  geom_stars(data = ok, aes(fill = var1.pred, x = x, y = y)) + 
  scale_fill_gradientn(colours = viridis(10), na.value = NA) + 
  coord_sf() + 
  scale_x_continuous(labels = scales::label_number(scale = 1/1000)) + 
  scale_y_continuous(labels = scales::label_number(scale = 1/1000)) +
  labs(x = x_map, y = y_map, 
       # title = 'Predicción', 
       fill = 'Predicción') +
  theme(legend.position = 'top')
```

```{r mapa-var, warning=FALSE, fig.cap='Mapa de varianza (error) de la variable de interés.', out.width='90%'}
gg.var = ggplot() + 
  geom_stars(data = ok, aes(fill = var1.var, x = x, y = y)) + 
  scale_fill_gradientn(colours = brewer.pal(9,'RdPu'), na.value = NA) + 
  coord_sf() + 
  scale_x_continuous(labels = scales::label_number(scale = 1/1000)) + 
  scale_y_continuous(labels = scales::label_number(scale = 1/1000)) +
  labs(x = x_map, y = y_map, 
       # title = 'Varianza',
       fill = 'Varianza') +
  theme(legend.position = 'top')
```

(ref:mapas-kriging) Mapas de predicción (**A**) y de la varianza/error (**B**) de la temperatura para la provincia de San José, para la fecha del 8 de Marzo.

```{r mapas-kriging, warning=FALSE, fig.cap='(ref:mapas-kriging)', out.width='100%', fig.height=6, fig.width=7}
gg.kriging = gg.pred + gg.var + 
  plot_annotation(tag_levels = 'A')
gg.kriging
```

# Aplicación web

Lo demostrado acá se encuentra implementado en una aplicación web [@garnier-villarreal2019c], la cual puede ser usada accediendo a la siguiente dirección <https://maximiliano-01.shinyapps.io/geostatistics/>. La idea de la aplicación es llevar de la mano al usuario por los mismos pasos presentados acá, usando una interfaz más familiar, sin necesidad de que sepa usar **R** o lenguajes de programación, pero sí es necesario que se entienda y tenga conciencia de lo que conlleva un análisis geoestadístico de principio a fin.

La aplicación puede leer (cargar) archivos '.txt' o '.csv', donde el archivo tiene que contener por lo menos tres columnas: coordenada-x, coordenada-y, variable de interés. La Figura \@ref(fig:webapp) muestra la interfaz de la aplicación.

(ref:webapp) Interfaz de la aplicación web para realizar análisis geoestadístico. El rectángulo amarillo encierra las viñetas (pasos) a seguir durante el análisis, incluyendo además la viñeta para desplegar los datos y la de información adicional de la aplicación.

```{r webapp, echo=FALSE, out.width='100%', fig.cap='(ref:webapp)'}
knitr::include_graphics(here('images','geostats-webapp.png'))
```

# Conclusiones

Kriging es un método de interpolación, de varios disponibles, para obtener predicciones (estimaciones) en puntos donde no se tienen observaciones, y adicionalmente presenta diferentes variantes, por lo que no es único y depende del objetivo de investigación el cómo se implementa. Cuando es posible y adecuado usarlo, típicamente, brinda los mejores resultados, además de proporcionar una estimación del error sobre los valores estimados.

Kriging como tal es uno de los posibles usos de la geoestadística, ya que es un paso (el último típicamente) durante un análisis geoestadístico donde el objetivo es la predicción (estimación) de una o varias variables en el espacio. Se menciona, brevemente, que otro posible producto de la geoestadística es la simulación, la cual puede ser más representativa en casos donde la heterogeneidad, y no el comportamiento promedio, de la variable es el interés principal.

La geoestadística, como rama de la estadística espacial, se enfoca en la caracterización de procesos y variables que tienen una fuerte componente espacial, por lo que existe una inter-dependencia entre las observaciones, a diferencia de la estadística clásica.

Este trabajo muestra los pasos, cuidados, y decisiones que hay que tomar durante un análisis geoestadístico típico, haciendo énfasis en que para obtener resultados válidos y confiables es necesario desarrollar estos pasos con criterio y no dejarlos a decisión de un programa de cómputo. Se recomienda que cuando se hace uso de Kriging se detalle el tipo, así como el modelo que se ajustó y sus parámetros.

En el ejemplo de la temperatura promedio de los últimos 10 años para el 8 de Marzo para la provincia de San José, se determinó la existencia de anisotropía con un eje mayor en dirección 135°, y una distribución de la temperatura donde valores altos se encuentran hacia el SW y valores bajos hacia el NE.

**R** es un lenguaje de programación muy flexible que permite crear rutinas para reusar posteriormente en análisis futuros similares. El material de este trabajo se encuentra disponible en un repositorio en GitHub (<https://github.com/maxgav13/intro_geostats>), que puede ser descargado para su uso. En el repositorio se puede consultar un material extra que presenta y detalla el código usado durante el análisis geoestadístico. Además se presenta de manera muy rápida una aplicación web que hace uso del mismo código, pero de una manera más amigable para quienes no se siente cómodos con lenguajes de programación.

# Referencias

```{r write-packages, include = FALSE}
if (!file.exists("bib/packages.bib")) file.create("bib/packages.bib")
if (!file.exists("bib/knit.bib")) file.create("bib/knit.bib")
suppressWarnings(
  knitr::write_bib(c("rmarkdown", "bookdown"), "bib/knit.bib")
)
suppressWarnings(
  knitr::write_bib(c(.packages()), "bib/packages.bib")
)
```

```{r write-bib, eval=TRUE, include=FALSE}
allref = c(readLines('bib/geostats.bib'),
           '\n',
           readLines('bib/knit.bib'),
           '\n',
           readLines('bib/packages.bib') %>% 
             str_replace('Manual','software') %>% 
             str_replace('note','version') %>% 
             str_replace('R package version','paquete de R'))
writeLines(allref,'bib/all.bib')
```
